{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d9fF26SqOvu",
        "outputId": "c7e35823-b9c8-4b01-aed2-122286e1d84a"
      },
      "outputs": [],
      "source": [
        "!pip install nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfg5M81Ns5OL",
        "outputId": "42c340c2-e90d-48a2-9a7d-fb177fa474c9"
      },
      "outputs": [],
      "source": [
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "777QTR52BSHL"
      },
      "source": [
        "A programe for vector addition. Reference: https://github.com/PacktPublishing/Learn-CUDA-Programming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKPt09aKs801",
        "outputId": "55bdec0e-bdff-44a1-938d-454019765bad"
      },
      "outputs": [],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <type_traits>\n",
        "\n",
        "#define TYPE int\n",
        "#define N 512\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void device_add(T *a, T *b, T *c)\n",
        "{\n",
        "\n",
        "    int id = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    c[id] = a[id] + b[id];\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "void print_output(T *a, T *b, T *c)\n",
        "{\n",
        "    for (int i = 0; i < N; ++i)\n",
        "    {\n",
        "        printf(\"\\n %d + %d  = %d\", a[i], b[i], c[i]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    TYPE *a, *b, *c;\n",
        "    TYPE *d_a, *d_b, *d_c;\n",
        "    int threads_per_block = 0, no_of_blocks = 0;\n",
        "\n",
        "    int size = N * sizeof(TYPE);\n",
        "\n",
        "    // Allocate space for host copies of a, b, c and setup input values\n",
        "    a = (TYPE *)malloc(size);\n",
        "    b = (TYPE *)malloc(size);\n",
        "    c = (TYPE *)malloc(size);\n",
        "\n",
        "    for (int i = 0; i < N; ++i)\n",
        "    {\n",
        "        a[i] = i;\n",
        "        b[i] = i;\n",
        "    }\n",
        "\n",
        "    // Allocate space for device copies of a, b, c\n",
        "    cudaMalloc((void **)&d_a, size);\n",
        "    cudaMalloc((void **)&d_b, size);\n",
        "    cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "    // Copy data from host to device\n",
        "    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    threads_per_block = 4;\n",
        "    no_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
        "    device_add<<<no_of_blocks, threads_per_block>>>(d_a, d_b, d_c);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    print_output(a, b, c);\n",
        "\n",
        "    free(a);\n",
        "    free(b);\n",
        "    free(c);\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
