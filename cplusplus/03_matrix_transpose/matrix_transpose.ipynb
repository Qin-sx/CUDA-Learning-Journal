{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### matrix_transpose_naive\n",
        "- **Description**: A simple implementation of matrix transposition without any optimization techniques.\n",
        "\n",
        "### matrix_transpose_shared_uncoalesced\n",
        "- **Description**: A matrix transposition method using shared memory and uncoalesced memory accesses in `output`.\n",
        "\n",
        "### matrix_transpose_shared_uncoalesced_no_conflict\n",
        "- **Description**: A matrix transposition method using shared memory and uncoalesced memory accesses, which avoids bank conflicts.\n",
        "\n",
        "### matrix_transpose_shared_coalesced\n",
        "- **Description**: A matrix transposition method using shared memory and coalesced memory accesses.\n",
        "\n",
        "### matrix_transpose_shared_coalesced_no_conflict (the fastest one)\n",
        "- **Description**: A matrix transposition method using shared memory and coalesced memory accesses, which avoids bank conflicts.\n",
        "\n",
        "### avoid bank conflict\n",
        "| bank 0 | bank 1 | bank 2 | bank 3 | ... | bank 30 | bank 31 |\n",
        "| :---------: | :----------: | :----------: | :---------: | :----------: | :----------: | :----------: |\n",
        "| 0 | 1 | 2 | 3 | ... | 30 | 31 |\n",
        "| NULL | 32 | 33 | 34 | ... | 61 | 62 |\n",
        "| 63 | NULL | 64 | 65 | ... | 92 | 94 |\n",
        "| 94 | 95 | NULL | 96 | ... | 123 | 124 |  \n",
        "\n",
        "when reading data in `sharedMemory`, the sequence will be 0, 32, 64 with no bank conflict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAKepX657d7X",
        "outputId": "d3da25e2-d647-4b02-90c8-f14f275bf96e"
      },
      "outputs": [],
      "source": [
        "%%writefile matrix_transpose.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <type_traits>\n",
        "#include <thrust/host_vector.h>\n",
        "#include <thrust/device_vector.h>\n",
        "#include <thrust/sequence.h>\n",
        "\n",
        "#define TYPE int\n",
        "#define N 640\n",
        "#define M 1280\n",
        "#define BLOCK_SIZE 32\n",
        "\n",
        "// 1 2 3 4\n",
        "// 5 6 7 8\n",
        "\n",
        "// 1 5\n",
        "// 2 6\n",
        "// 3 7\n",
        "// 4 8\n",
        "__global__ void warm_up() {\n",
        "    int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\t  int indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    if(indexX < N && indexY < M)\n",
        "    {\n",
        "        float a = 0.0f;\n",
        "        float b = 1.0f;\n",
        "        float c = a + b;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void matrix_transpose_naive(T *input, T *output) {\n",
        "\n",
        "\tint indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\tint indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "\n",
        "  if (indexX < N && indexY < M)\n",
        "  {\n",
        "      int index = indexY * N + indexX;\n",
        "      int transposedIndex = indexX * M + indexY;\n",
        "\n",
        "      // this has discoalesced global memory store\n",
        "      output[transposedIndex] = input[index];\n",
        "\n",
        "      // this has discoalesced global memore load\n",
        "      // output[index] = input[transposedIndex];\n",
        "      // printf(\"%d, %d, %d, %d, %d \\n\", indexX, indexY, index, transposedIndex, input[index]);\n",
        "  }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void matrix_transpose_shared_uncoalesed(T *input, T *output) {\n",
        "\n",
        "\t__shared__ T sharedMemory [BLOCK_SIZE] [BLOCK_SIZE];\n",
        "\n",
        "\t// global index\n",
        "\tint indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\tint indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "  if (indexX < N && indexY < M)\n",
        "  {\n",
        "      int index = indexY * N + indexX;\n",
        "      int transposedIndex = indexX * M + indexY;\n",
        "\n",
        "\t    // local index\n",
        "\t    int localIndexX = threadIdx.x;\n",
        "\t    int localIndexY = threadIdx.y;\n",
        "\n",
        "\t    // reading from global memory in coalesed manner and performing tanspose in shared memory\n",
        "\t    sharedMemory[localIndexX][localIndexY] = input[index];\n",
        "\n",
        "\t    __syncthreads();\n",
        "\n",
        "\t    // writing into global memory in coalesed fashion via transposed data in shared memory\n",
        "\t    output[transposedIndex] = sharedMemory[localIndexX][localIndexY];\n",
        "  }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void matrix_transpose_shared_uncoalesed_no_conflict(T *input, T *output) {\n",
        "\n",
        "\t__shared__ T sharedMemory [BLOCK_SIZE] [BLOCK_SIZE + 1];\n",
        "\n",
        "\t// global index\n",
        "\tint indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\tint indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "  if (indexX < N && indexY < M)\n",
        "  {\n",
        "      int index = indexY * N + indexX;\n",
        "      int transposedIndex = indexX * M + indexY;\n",
        "\n",
        "\t    // local index\n",
        "\t    int localIndexX = threadIdx.x;\n",
        "\t    int localIndexY = threadIdx.y;\n",
        "\n",
        "\t    // reading from global memory in coalesed manner and performing tanspose in shared memory\n",
        "\t    sharedMemory[localIndexX][localIndexY] = input[index];\n",
        "\n",
        "\t    __syncthreads();\n",
        "\n",
        "\t    // writing into global memory in coalesed fashion via transposed data in shared memory\n",
        "\t    output[transposedIndex] = sharedMemory[localIndexX][localIndexY];\n",
        "  }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void matrix_transpose_shared_coalesed(T *input, T *output) {\n",
        "\n",
        "\t__shared__ T sharedMemory [BLOCK_SIZE] [BLOCK_SIZE];\n",
        "\n",
        "\t// global index\n",
        "\tint indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\tint indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "  // local index\n",
        "\tint localIndexX = threadIdx.x;\n",
        "\tint localIndexY = threadIdx.y;\n",
        "\n",
        "  if (indexX < N && indexY < M)\n",
        "  {\n",
        "      int index = indexY * N + indexX;\n",
        "      \n",
        "\t    // reading from global memory in coalesed manner and performing tanspose in shared memory\n",
        "\t    sharedMemory[localIndexX][localIndexY] = input[index];\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  // transposed global memory index\n",
        "\tint tindexX = threadIdx.x + blockIdx.y * blockDim.x;\n",
        "\tint tindexY = threadIdx.y + blockIdx.x * blockDim.y;\n",
        "  if(tindexX < M && tindexY < N)\n",
        "  {\n",
        "      int transposedIndex = tindexY * M + tindexX;\n",
        "      // writing into global memory in coalesed fashion via transposed data in shared memory\n",
        "\t    output[transposedIndex] = sharedMemory[localIndexY][localIndexX];\n",
        "  }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void matrix_transpose_shared_coalesed_no_conflict(T *input, T *output) {\n",
        "\n",
        "\t__shared__ T sharedMemory [BLOCK_SIZE] [BLOCK_SIZE + 1];\n",
        "\n",
        "\t// global index\n",
        "\tint indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\tint indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "  // local index\n",
        "\tint localIndexX = threadIdx.x;\n",
        "\tint localIndexY = threadIdx.y;\n",
        "\n",
        "  if (indexX < N && indexY < M)\n",
        "  {\n",
        "      int index = indexY * N + indexX;\n",
        "      \n",
        "\t    // reading from global memory in coalesed manner and performing tanspose in shared memory\n",
        "\t    sharedMemory[localIndexX][localIndexY] = input[index];\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  // transposed global memory index\n",
        "\tint tindexX = threadIdx.x + blockIdx.y * blockDim.x;\n",
        "\tint tindexY = threadIdx.y + blockIdx.x * blockDim.y;\n",
        "  if(tindexX < M && tindexY < N)\n",
        "  {\n",
        "      int transposedIndex = tindexY * M + tindexX;\n",
        "      // writing into global memory in coalesed fashion via transposed data in shared memory\n",
        "\t    output[transposedIndex] = sharedMemory[localIndexY][localIndexX];\n",
        "  }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "void print_output(T *a, T *b)\n",
        "{\n",
        "    for (int i = 0; i < N * M; ++i)\n",
        "    {\n",
        "        if (i % N == 0)\n",
        "        {\n",
        "            std::cout<<std::endl;\n",
        "        }\n",
        "        std::cout<<a[i]<<\" \";\n",
        "    }\n",
        "    std::cout<<std::endl;\n",
        "    for (int i = 0; i < N * M; ++i)\n",
        "    {\n",
        "        if (i % M == 0)\n",
        "        {\n",
        "            std::cout<<std::endl;\n",
        "        }\n",
        "        std::cout<<b[i]<<\" \";\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    // Allocate space for host copies of a, b\n",
        "    thrust::host_vector<TYPE> a(N*M);\n",
        "    thrust::host_vector<TYPE> b(N*M);\n",
        "\n",
        "    // Allocate space for device copies of a, b\n",
        "    thrust::device_vector<TYPE> d_a(N*M, 0);\n",
        "    thrust::device_vector<TYPE> d_b(N*M, 0);\n",
        "    thrust::sequence(d_a.begin(), d_a.end(), 0, 1);\n",
        "\n",
        "    dim3 threads_per_block(BLOCK_SIZE, BLOCK_SIZE, 1);\n",
        "    dim3 no_of_blocks((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (M + BLOCK_SIZE - 1) / BLOCK_SIZE, 1);\n",
        "\n",
        "    warm_up<<<no_of_blocks, threads_per_block>>>();\n",
        "    matrix_transpose_naive<<<no_of_blocks, threads_per_block>>>(thrust::raw_pointer_cast(d_a.data()),thrust::raw_pointer_cast(d_b.data()));\n",
        "    matrix_transpose_shared_uncoalesed<<<no_of_blocks, threads_per_block>>>(thrust::raw_pointer_cast(d_a.data()),thrust::raw_pointer_cast(d_b.data()));\n",
        "    matrix_transpose_shared_uncoalesed_no_conflict<<<no_of_blocks, threads_per_block>>>(thrust::raw_pointer_cast(d_a.data()),thrust::raw_pointer_cast(d_b.data()));\n",
        "    matrix_transpose_shared_coalesed<<<no_of_blocks, threads_per_block>>>(thrust::raw_pointer_cast(d_a.data()),thrust::raw_pointer_cast(d_b.data()));\n",
        "    matrix_transpose_shared_coalesed_no_conflict<<<no_of_blocks, threads_per_block>>>(thrust::raw_pointer_cast(d_a.data()),thrust::raw_pointer_cast(d_b.data()));\n",
        "\n",
        "    thrust::copy(d_a.begin(), d_a.end(), a.begin());\n",
        "    thrust::copy(d_b.begin(), d_b.end(), b.begin());\n",
        "\n",
        "    // print_output(a.data(), b.data());\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIyLf1nu8KPf",
        "outputId": "d7b6e0c2-4590-4f80-9e7e-94b536f6b69d"
      },
      "outputs": [],
      "source": [
        "!nvcc -o matrix_transpose matrix_transpose.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXtlEDJj9GJ5"
      },
      "outputs": [],
      "source": [
        "!./matrix_transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auvpVpsuF47s",
        "outputId": "c9630411-827e-4512-bcf2-eab03de8da47"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lw8oX_UF-SM"
      },
      "outputs": [],
      "source": [
        "!apt update\n",
        "!apt install ./drive/MyDrive/Nsight/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt --fix-broken install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKDAvLA1GJPW",
        "outputId": "e616628d-c952-46a5-a11f-93c4d6c8b6b0"
      },
      "outputs": [],
      "source": [
        "!nsys profile -o report_matrix_transpose ./matrix_transpose"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
