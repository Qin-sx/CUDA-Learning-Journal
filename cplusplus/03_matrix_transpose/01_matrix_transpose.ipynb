{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzEKO5FJZk2m"
      },
      "source": [
        "### matrix_transpose_naive\n",
        "- **Description**: A simple implementation of matrix transposition without any optimization techniques.\n",
        "\n",
        "### matrix_transpose_shared_uncoalesced\n",
        "- **Description**: A matrix transposition method using shared memory and uncoalesced memory accesses in `output`.\n",
        "\n",
        "### matrix_transpose_shared_uncoalesced_no_conflict\n",
        "- **Description**: A matrix transposition method using shared memory and uncoalesced memory accesses, which avoids bank conflicts.\n",
        "\n",
        "### matrix_transpose_shared_coalesced\n",
        "- **Description**: A matrix transposition method using shared memory and coalesced memory accesses.\n",
        "\n",
        "### matrix_transpose_shared_coalesced_no_conflict (the fastest one)\n",
        "- **Description**: A matrix transposition method using shared memory and coalesced memory accesses, which avoids bank conflicts.\n",
        "\n",
        "### avoid bank conflict\n",
        "| bank 0 | bank 1 | bank 2 | bank 3 | ... | bank 30 | bank 31 |\n",
        "| :---------: | :----------: | :----------: | :---------: | :----------: | :----------: | :----------: |\n",
        "| 0 | 1 | 2 | 3 | ... | 30 | 31 |\n",
        "| NULL | 32 | 33 | 34 | ... | 61 | 62 |\n",
        "| 63 | NULL | 64 | 65 | ... | 92 | 94 |\n",
        "| 94 | 95 | NULL | 96 | ... | 123 | 124 |  \n",
        "\n",
        "when reading data in `sharedMemory`, the sequence will be 0, 32, 64 with no bank conflict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAKepX657d7X",
        "outputId": "b5a4130b-fcea-4757-dc9f-624ce19e2bb8"
      },
      "outputs": [],
      "source": [
        "%%writefile matrix_transpose.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <type_traits>\n",
        "#include <thrust/host_vector.h>\n",
        "#include <thrust/device_vector.h>\n",
        "#include <thrust/sequence.h>\n",
        "\n",
        "#define TYPE int\n",
        "#define N 5120\n",
        "#define M 6400\n",
        "#define BLOCK_SIZE 32\n",
        "#define NUM_PER_THREAD 8\n",
        "\n",
        "// 1 2 3 4\n",
        "// 5 6 7 8\n",
        "\n",
        "// 1 5\n",
        "// 2 6\n",
        "// 3 7\n",
        "// 4 8\n",
        "__global__ void warm_up()\n",
        "{\n",
        "    int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    if (indexX < N && indexY < M)\n",
        "    {\n",
        "        float a = 0.0f;\n",
        "        float b = 1.0f;\n",
        "        float c = a + b;\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void matrix_transpose_naive(T *input, T *output)\n",
        "{\n",
        "\n",
        "    int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "\n",
        "    if (indexX < N && indexY < M)\n",
        "    {\n",
        "        int index = indexY * N + indexX;\n",
        "        int transposedIndex = indexX * M + indexY;\n",
        "\n",
        "        // this has discoalesced global memory store\n",
        "        output[transposedIndex] = input[index];\n",
        "\n",
        "        // this has discoalesced global memore load\n",
        "        // output[index] = input[transposedIndex];\n",
        "        // printf(\"%d, %d, %d, %d, %d \\n\", indexX, indexY, index, transposedIndex, input[index]);\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void matrix_transpose_shared_uncoalesed(T *input, T *output)\n",
        "{\n",
        "\n",
        "    __shared__ T sharedMemory[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    // global index\n",
        "    int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    if (indexX < N && indexY < M)\n",
        "    {\n",
        "        int index = indexY * N + indexX;\n",
        "        int transposedIndex = indexX * M + indexY;\n",
        "\n",
        "        // local index\n",
        "        int localIndexX = threadIdx.x;\n",
        "        int localIndexY = threadIdx.y;\n",
        "\n",
        "        // reading from global memory in coalesed manner and performing tanspose in shared memory\n",
        "        sharedMemory[localIndexX][localIndexY] = input[index];\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        // writing into global memory in coalesed fashion via transposed data in shared memory\n",
        "        output[transposedIndex] = sharedMemory[localIndexX][localIndexY];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void matrix_transpose_shared_uncoalesed_no_conflict(T *input, T *output)\n",
        "{\n",
        "\n",
        "    __shared__ T sharedMemory[BLOCK_SIZE][BLOCK_SIZE + 1];\n",
        "\n",
        "    // global index\n",
        "    int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    if (indexX < N && indexY < M)\n",
        "    {\n",
        "        int index = indexY * N + indexX;\n",
        "        int transposedIndex = indexX * M + indexY;\n",
        "\n",
        "        // local index\n",
        "        int localIndexX = threadIdx.x;\n",
        "        int localIndexY = threadIdx.y;\n",
        "\n",
        "        // reading from global memory in coalesed manner and performing tanspose in shared memory\n",
        "        sharedMemory[localIndexX][localIndexY] = input[index];\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        // writing into global memory in coalesed fashion via transposed data in shared memory\n",
        "        output[transposedIndex] = sharedMemory[localIndexX][localIndexY];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void matrix_transpose_shared_coalesed(T *input, T *output)\n",
        "{\n",
        "\n",
        "    __shared__ T sharedMemory[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    // global index\n",
        "    int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    // local index\n",
        "    int localIndexX = threadIdx.x;\n",
        "    int localIndexY = threadIdx.y;\n",
        "\n",
        "    if (indexX < N && indexY < M)\n",
        "    {\n",
        "        int index = indexY * N + indexX;\n",
        "\n",
        "        // reading from global memory in coalesed manner and performing tanspose in shared memory\n",
        "        sharedMemory[localIndexX][localIndexY] = input[index];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // transposed global memory index\n",
        "    int tindexX = threadIdx.x + blockIdx.y * blockDim.x;\n",
        "    int tindexY = threadIdx.y + blockIdx.x * blockDim.y;\n",
        "    if (tindexX < M && tindexY < N)\n",
        "    {\n",
        "        int transposedIndex = tindexY * M + tindexX;\n",
        "        // writing into global memory in coalesed fashion via transposed data in shared memory\n",
        "        output[transposedIndex] = sharedMemory[localIndexY][localIndexX];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void matrix_transpose_shared_coalesed_no_conflict(T *input, T *output)\n",
        "{\n",
        "\n",
        "    __shared__ T sharedMemory[BLOCK_SIZE][BLOCK_SIZE + 1];\n",
        "\n",
        "    // global index\n",
        "    int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int indexY = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    // local index\n",
        "    int localIndexX = threadIdx.x;\n",
        "    int localIndexY = threadIdx.y;\n",
        "\n",
        "    if (indexX < N && indexY < M)\n",
        "    {\n",
        "        int index = indexY * N + indexX;\n",
        "\n",
        "        // reading from global memory in coalesed manner and performing tanspose in shared memory\n",
        "        sharedMemory[localIndexX][localIndexY] = input[index];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // transposed global memory index\n",
        "    int tindexX = threadIdx.x + blockIdx.y * blockDim.x;\n",
        "    int tindexY = threadIdx.y + blockIdx.x * blockDim.y;\n",
        "    if (tindexX < M && tindexY < N)\n",
        "    {\n",
        "        int transposedIndex = tindexY * M + tindexX;\n",
        "        // writing into global memory in coalesed fashion via transposed data in shared memory\n",
        "        output[transposedIndex] = sharedMemory[localIndexY][localIndexX];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void matrix_transpose_multi_elem(T *input, T *output)\n",
        "{\n",
        "\n",
        "    __shared__ T sharedMemory[BLOCK_SIZE][BLOCK_SIZE + 1];\n",
        "\n",
        "    // global index\n",
        "    int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int indexY = threadIdx.y + blockIdx.y * BLOCK_SIZE;\n",
        "    // local index\n",
        "    int localIndexX = threadIdx.x;\n",
        "    int localIndexY = threadIdx.y;\n",
        "\n",
        "    int row_stride = BLOCK_SIZE / NUM_PER_THREAD;\n",
        "\n",
        "    if (indexX < N)\n",
        "    {\n",
        "        #pragma unroll\n",
        "        for (int yOffset = 0; yOffset < BLOCK_SIZE; yOffset += row_stride)\n",
        "        {\n",
        "            int indexYNew = indexY + yOffset;\n",
        "            if (indexYNew < M)\n",
        "            {\n",
        "                int index = indexYNew * N + indexX;\n",
        "\n",
        "                // reading from global memory in coalesed manner and performing tanspose in shared memory\n",
        "                sharedMemory[localIndexX][localIndexY + yOffset] = input[index];\n",
        "                // if(index == 1024 || index == 992)\n",
        "                // {\n",
        "                //   printf(\"index = %d, input[index] = %d, indexX = %d, indexYNew = %d, localIndexX = %d, localIndexY = %d, localIndexY + yOffset= %d, yOffset = %d, blockIdx.x = %d, blockIdx.y = %d\\n\",\n",
        "                //       index, input[index], indexX, indexYNew, localIndexX, localIndexY, localIndexY + yOffset, yOffset, blockIdx.x, blockIdx.y);\n",
        "                // }\n",
        "            }\n",
        "\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // transposed global memory index\n",
        "    int tindexX = threadIdx.x + blockIdx.y * blockDim.x;\n",
        "    int tindexY = threadIdx.y + blockIdx.x * BLOCK_SIZE;\n",
        "    if (tindexX < M)\n",
        "    {\n",
        "        #pragma unroll\n",
        "        for (int yOffset = 0; yOffset < BLOCK_SIZE; yOffset += row_stride)\n",
        "        {\n",
        "            int tindexYNew = tindexY + yOffset;\n",
        "            if(tindexYNew < N)\n",
        "            {\n",
        "                int transposedIndex = tindexYNew * M + tindexX;\n",
        "                // writing into global memory in coalesed fashion via transposed data in shared memory\n",
        "                output[transposedIndex] = sharedMemory[localIndexY + yOffset][localIndexX];\n",
        "            }\n",
        "\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "void print_output(T *a, T *b)\n",
        "{\n",
        "    for (int i = 0; i < N * M; ++i)\n",
        "    {\n",
        "        if (i % N == 0)\n",
        "        {\n",
        "            std::cout << std::endl;\n",
        "        }\n",
        "        std::cout << a[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "    for (int i = 0; i < N * M; ++i)\n",
        "    {\n",
        "        if (i % M == 0)\n",
        "        {\n",
        "            std::cout << std::endl;\n",
        "        }\n",
        "        std::cout << b[i] << \" \";\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    // Allocate space for host copies of a, b\n",
        "    thrust::host_vector<TYPE> a(N * M);\n",
        "    thrust::host_vector<TYPE> b(N * M);\n",
        "\n",
        "    // Allocate space for device copies of a, b\n",
        "    thrust::device_vector<TYPE> d_a(N * M, 0);\n",
        "    thrust::device_vector<TYPE> d_b(N * M, 0);\n",
        "    thrust::sequence(d_a.begin(), d_a.end(), 0, 1);\n",
        "\n",
        "    dim3 threads_per_block(BLOCK_SIZE, BLOCK_SIZE, 1);\n",
        "    dim3 no_of_blocks((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (M + BLOCK_SIZE - 1) / BLOCK_SIZE, 1);\n",
        "\n",
        "    warm_up<<<no_of_blocks, threads_per_block>>>();\n",
        "    matrix_transpose_naive<<<no_of_blocks, threads_per_block>>>(thrust::raw_pointer_cast(d_a.data()), thrust::raw_pointer_cast(d_b.data()));\n",
        "    matrix_transpose_shared_uncoalesed<<<no_of_blocks, threads_per_block>>>(thrust::raw_pointer_cast(d_a.data()), thrust::raw_pointer_cast(d_b.data()));\n",
        "    matrix_transpose_shared_uncoalesed_no_conflict<<<no_of_blocks, threads_per_block>>>(thrust::raw_pointer_cast(d_a.data()), thrust::raw_pointer_cast(d_b.data()));\n",
        "    matrix_transpose_shared_coalesed<<<no_of_blocks, threads_per_block>>>(thrust::raw_pointer_cast(d_a.data()), thrust::raw_pointer_cast(d_b.data()));\n",
        "    matrix_transpose_shared_coalesed_no_conflict<<<no_of_blocks, threads_per_block>>>(thrust::raw_pointer_cast(d_a.data()), thrust::raw_pointer_cast(d_b.data()));\n",
        "\n",
        "    dim3 threads_per_block_multielem(BLOCK_SIZE, int(BLOCK_SIZE / NUM_PER_THREAD), 1);\n",
        "    // std::cout <<\"no_of_blocks\"<<no_of_blocks.x <<\" \"<<no_of_blocks.y<<\" \"<<no_of_blocks.z<< std::endl;\n",
        "    matrix_transpose_multi_elem<<<no_of_blocks, threads_per_block_multielem>>>(thrust::raw_pointer_cast(d_a.data()), thrust::raw_pointer_cast(d_b.data()));\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    thrust::copy(d_a.begin(), d_a.end(), a.begin());\n",
        "    thrust::copy(d_b.begin(), d_b.end(), b.begin());\n",
        "\n",
        "    // print_output(a.data(), b.data());\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIyLf1nu8KPf",
        "outputId": "5cdce416-77d4-428d-9913-8b824245c046"
      },
      "outputs": [],
      "source": [
        "!nvcc -o matrix_transpose -lineinfo matrix_transpose.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXtlEDJj9GJ5"
      },
      "outputs": [],
      "source": [
        "!./matrix_transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auvpVpsuF47s",
        "outputId": "e3a4ad7b-9d51-4341-98cc-5cc10eaa22ff"
      },
      "outputs": [],
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.4.4_2023.4.4.54-1_amd64.deb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lw8oX_UF-SM",
        "outputId": "a0386b8b-94e7-4d17-8a05-21607150699d"
      },
      "outputs": [],
      "source": [
        "!apt update\n",
        "!apt install ./nsight-systems-2023.4.4_2023.4.4.54-1_amd64.deb\n",
        "!apt --fix-broken install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKDAvLA1GJPW",
        "outputId": "2e987a08-cb96-4857-d7d1-7e69515cfe70"
      },
      "outputs": [],
      "source": [
        "!nsys profile -o report_nsys_matrix_transpose ./matrix_transpose -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVisVB9qZk2q",
        "outputId": "5993e7b4-5022-40fc-a0a9-f3753c68202c"
      },
      "outputs": [],
      "source": [
        "!ncu --set full --replay-mode kernel --target-processes all -o report_ncu_matrix_transpose -f ./matrix_transpose"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
