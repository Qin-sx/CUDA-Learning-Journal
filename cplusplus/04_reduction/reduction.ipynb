{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcM15VkcrGIr"
      },
      "source": [
        "Reference: https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf, https://zhuanlan.zhihu.com/p/426978026"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "reduce2: Avoid bank conflict in reduce1.  \n",
        "reduce7: The final step has been replaced by utilizing a shuffle operation, but no significant performance enhancement has been observed.  \n",
        "reduce8: Due to the global memory and bank conflict restriction, we cannot process data with divid-and-conquer strategy. We can just set block size equal to warp size for avoiding bank conflict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ58WigeRD37",
        "outputId": "1ae023bd-2a4f-4e79-8ca6-bb1c6ac129e6"
      },
      "outputs": [],
      "source": [
        "%%writefile reduction.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <type_traits>\n",
        "#include <thrust/host_vector.h>\n",
        "#include <thrust/device_vector.h>\n",
        "#include <thrust/sequence.h>\n",
        "\n",
        "#define TYPE int\n",
        "#define N 42990\n",
        "#define BLOCK_SIZE 1024\n",
        "#define NUM_PER_THREAD 8\n",
        "#define WARP_SIZE (BLOCK_SIZE / NUM_PER_THREAD / 32)\n",
        "\n",
        "__global__ void  warm_up()\n",
        "{\n",
        "    int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (indexX < N)\n",
        "    {\n",
        "        float a = 0.0f;\n",
        "        float b = 1.0f;\n",
        "        float c = a + b;\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void reduce_0(T *input, T *output, int size)\n",
        "{\n",
        "    extern __shared__ T sdata[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (index < size)\n",
        "    {\n",
        "        sdata[tid] = input[index];\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        sdata[tid] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int s = 1; s < blockDim.x; s *= 2)\n",
        "    {\n",
        "        if (tid % (2 * s) == 0)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0)\n",
        "    {\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void reduce_1(T *input, T *output, int size)\n",
        "{\n",
        "    extern __shared__ T sdata[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (index < size)\n",
        "    {\n",
        "        sdata[tid] = input[index];\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        sdata[tid] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int s = 1; s < blockDim.x; s *= 2)\n",
        "    {\n",
        "        int index = 2 * s * tid;\n",
        "        if (index < blockDim.x)\n",
        "        {\n",
        "            sdata[index] += sdata[index + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0)\n",
        "    {\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void reduce_2(T *input, T *output, int size)\n",
        "{\n",
        "    extern __shared__ T sdata[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (index < size)\n",
        "    {\n",
        "        sdata[tid] = input[index];\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        sdata[tid] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1)\n",
        "    {\n",
        "        if (tid < s)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0)\n",
        "    {\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void reduce_3(T *input, T *output, int size)\n",
        "{\n",
        "    extern __shared__ T sdata[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int index = blockIdx.x * (blockDim.x * 2) + threadIdx.x;\n",
        "\n",
        "    if (index + blockDim.x < size)\n",
        "    {\n",
        "        sdata[tid] = input[index] + input[index + blockDim.x];\n",
        "    }\n",
        "    else if (index + blockDim.x >= size && index < size)\n",
        "    {\n",
        "        sdata[tid] = input[index];\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        sdata[tid] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1)\n",
        "    {\n",
        "        if (tid < s)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0)\n",
        "    {\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <int Block_Size_T, typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__device__ void warpReduce(volatile T *sdata, int tid)\n",
        "{\n",
        "    if constexpr (Block_Size_T >= 64) sdata[tid] += sdata[tid + 32];\n",
        "    if constexpr (Block_Size_T >= 32) sdata[tid] += sdata[tid + 16];\n",
        "    if constexpr (Block_Size_T >= 16) sdata[tid] += sdata[tid + 8];\n",
        "    if constexpr (Block_Size_T >= 8) sdata[tid] += sdata[tid + 4];\n",
        "    if constexpr (Block_Size_T >= 4) sdata[tid] += sdata[tid + 2];\n",
        "    if constexpr (Block_Size_T >= 2) sdata[tid] += sdata[tid + 1];\n",
        "}\n",
        "\n",
        "template <int Block_Size_T, typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void reduce_4(T *input, T *output, int size)\n",
        "{\n",
        "    extern __shared__ T sdata[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int index = blockIdx.x * (blockDim.x * 2) + threadIdx.x;\n",
        "\n",
        "    if (index + blockDim.x < size)\n",
        "    {\n",
        "        sdata[tid] = input[index] + input[index + blockDim.x];\n",
        "    }\n",
        "    else if (index + blockDim.x >= size && index < size)\n",
        "    {\n",
        "        sdata[tid] = input[index];\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        sdata[tid] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int s = blockDim.x / 2; s > 32; s >>= 1)\n",
        "    {\n",
        "        if (tid < s)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        warpReduce<Block_Size_T>(sdata, tid);\n",
        "    }\n",
        "\n",
        "    if (tid == 0)\n",
        "    {\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <int Block_Size_T, typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void reduce_5(T *input, T *output, int size)\n",
        "{\n",
        "    extern __shared__ T sdata[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int index = blockIdx.x * (blockDim.x * 2) + threadIdx.x;\n",
        "\n",
        "    if (index + blockDim.x < size)\n",
        "    {\n",
        "        sdata[tid] = input[index] + input[index + blockDim.x];\n",
        "    }\n",
        "    else if (index + blockDim.x >= size && index < size)\n",
        "    {\n",
        "        sdata[tid] = input[index];\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        sdata[tid] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    if constexpr (Block_Size_T / 2 >= 512)\n",
        "    {\n",
        "        if (tid < 256)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + 256];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if constexpr (Block_Size_T / 2 >= 256)\n",
        "    {\n",
        "        if (tid < 128)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + 128];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if constexpr (Block_Size_T / 2 >= 128)\n",
        "    {\n",
        "        if (tid < 64)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + 64];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        warpReduce<Block_Size_T / 2>(sdata, tid);\n",
        "    }\n",
        "\n",
        "    if (tid == 0)\n",
        "    {\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "template <int Block_Size_T, int NUM_PER_THREAD_T, typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void reduce_6(T *input, T *output, int size)\n",
        "{\n",
        "    extern __shared__ T sdata[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int index = blockIdx.x * (blockDim.x * NUM_PER_THREAD_T) + threadIdx.x;\n",
        "    sdata[tid] = 0;\n",
        "\n",
        "#pragma unroll\n",
        "    for (int stride = 0; stride < Block_Size_T; stride += int(Block_Size_T / NUM_PER_THREAD_T))\n",
        "    {\n",
        "        int idTmp = index + stride;\n",
        "        if(idTmp < size) sdata[tid] += input[idTmp];\n",
        "    }\n",
        "    //printf(\"%d, data1 = %d \\n\", tid, sdata[tid]);\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if constexpr (Block_Size_T / NUM_PER_THREAD_T >= 512)\n",
        "    {\n",
        "        if (tid < 256)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + 256];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if constexpr (Block_Size_T / NUM_PER_THREAD_T >= 256)\n",
        "    {\n",
        "        if (tid < 128)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + 128];\n",
        "            //printf(\"%d, data2 = %d \\n\", tid, sdata[tid]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if constexpr (Block_Size_T / NUM_PER_THREAD_T >= 128)\n",
        "    {\n",
        "        if (tid < 64)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + 64];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        warpReduce<int(Block_Size_T / NUM_PER_THREAD_T)>(sdata, tid);\n",
        "    }\n",
        "\n",
        "    if (tid == 0)\n",
        "    {\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "template <int Block_Size_T, typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__device__ void warpReduceShuffle(T *sdata, int tid)\n",
        "{\n",
        "    if constexpr (Block_Size_T >= 32) sdata[tid] +=__shfl_down_sync(0xffffffff,sdata[tid],16);\n",
        "    if constexpr (Block_Size_T >= 16) sdata[tid] +=__shfl_down_sync(0xffffffff,sdata[tid],8);\n",
        "    if constexpr (Block_Size_T >= 8) sdata[tid] +=__shfl_down_sync(0xffffffff,sdata[tid],4);\n",
        "    if constexpr (Block_Size_T >= 4) sdata[tid] +=__shfl_down_sync(0xffffffff,sdata[tid],2);\n",
        "    if constexpr (Block_Size_T >= 2) sdata[tid] +=__shfl_down_sync(0xffffffff,sdata[tid],1);\n",
        "}\n",
        "\n",
        "template <int Block_Size_T, typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__device__ __forceinline__ T warpReduceShuffle2(T sum)\n",
        "{\n",
        "    if constexpr (Block_Size_T >= 32) sum +=__shfl_down_sync(0xffffffff,sum,16);\n",
        "    if constexpr (Block_Size_T >= 16) sum +=__shfl_down_sync(0xffffffff,sum,8);\n",
        "    if constexpr (Block_Size_T >= 8) sum +=__shfl_down_sync(0xffffffff,sum,4);\n",
        "    if constexpr (Block_Size_T >= 4) sum +=__shfl_down_sync(0xffffffff,sum,2);\n",
        "    if constexpr (Block_Size_T >= 2) sum +=__shfl_down_sync(0xffffffff,sum,1);\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "template <int Block_Size_T, int NUM_PER_THREAD_T, typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void reduce_7(T *input, T *output, int size)\n",
        "{\n",
        "    extern __shared__ T sdata[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int index = blockIdx.x * (blockDim.x * NUM_PER_THREAD_T) + threadIdx.x;\n",
        "    sdata[tid] = 0;\n",
        "\n",
        "#pragma unroll\n",
        "    for (int stride = 0; stride < Block_Size_T; stride += int(Block_Size_T / NUM_PER_THREAD_T))\n",
        "    {\n",
        "        int idTmp = index + stride;\n",
        "        if(idTmp < size) sdata[tid] += input[idTmp];\n",
        "    }\n",
        "    //printf(\"%d, data1 = %d \\n\", tid, sdata[tid]);\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if constexpr (Block_Size_T / NUM_PER_THREAD_T >= 512)\n",
        "    {\n",
        "        if (tid < 256)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + 256];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if constexpr (Block_Size_T / NUM_PER_THREAD_T >= 256)\n",
        "    {\n",
        "        if (tid < 128)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + 128];\n",
        "            //printf(\"%d, data2 = %d \\n\", tid, sdata[tid]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if constexpr (Block_Size_T / NUM_PER_THREAD_T >= 128)\n",
        "    {\n",
        "        if (tid < 64)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + 64];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if constexpr (Block_Size_T / NUM_PER_THREAD_T >= 64)\n",
        "    {\n",
        "        if (tid < 32)\n",
        "        {\n",
        "            sdata[tid] += sdata[tid + 32];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // if (tid < 32)\n",
        "    // {\n",
        "    //     warpReduceShuffle<int(Block_Size_T / NUM_PER_THREAD_T)>(sdata, tid);\n",
        "    // }\n",
        "    // if (tid == 0)\n",
        "    // {\n",
        "    //     output[blockIdx.x] = sdata[0];\n",
        "    // }\n",
        "\n",
        "    T sum = 0;\n",
        "\n",
        "    if (tid < 32)\n",
        "    {\n",
        "        sum = sdata[tid];\n",
        "        sum = warpReduceShuffle2<int(Block_Size_T / NUM_PER_THREAD_T)>(sum);\n",
        "    }\n",
        "\n",
        "    if (tid == 0)\n",
        "    {\n",
        "        output[blockIdx.x] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "template <int Block_Size_T, int NUM_PER_THREAD_T, typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
        "__global__ void reduce_8(T *input, T *output, int size)\n",
        "{\n",
        "    T sum = 0;\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int index = blockIdx.x * (blockDim.x * NUM_PER_THREAD_T) + threadIdx.x;\n",
        "\n",
        "#pragma unroll\n",
        "    for (int stride = 0; stride < Block_Size_T; stride += int(Block_Size_T / NUM_PER_THREAD_T))\n",
        "    {\n",
        "        int idTmp = index + stride;\n",
        "        if(idTmp < size) sum += input[idTmp];\n",
        "    }\n",
        "    //printf(\"%d, data1 = %d \\n\", tid, sdata[tid]);\n",
        "\n",
        "    __syncthreads();\n",
        "    // WARP_SIZE = blockDim.x / 32; WARP_SIZE = BLOCK_SIZE / NUM_PER_THREAD / 32;\n",
        "    static __shared__ T warpLevelSums[WARP_SIZE]; // 2048 / 32 = 64 not valid!\n",
        "    const int laneId = threadIdx.x % 32; //warp 0 0, 1, 2, 3... warp 1 0, 1, 2, 3...\n",
        "    const int warpId = threadIdx.x / 32; //warp 0, warp 1, warp 2, warp 3...\n",
        "\n",
        "    sum = warpReduceShuffle2<int(Block_Size_T / NUM_PER_THREAD_T)>(sum);\n",
        "\n",
        "    if (laneId == 0)\n",
        "    {\n",
        "        warpLevelSums[warpId] = sum;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    assert(WARP_SIZE <= 32);\n",
        "    sum = (threadIdx.x < WARP_SIZE) ? warpLevelSums[laneId] : 0;\n",
        "\n",
        "    assert(WARP_SIZE <= 32);\n",
        "    if (warpId == 0)\n",
        "    {\n",
        "        sum = warpReduceShuffle2<int(WARP_SIZE)>(sum);\n",
        "    }\n",
        "\n",
        "    if (tid == 0)\n",
        "    {\n",
        "        output[blockIdx.x] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main()\n",
        "{\n",
        "    TYPE h_input[N];\n",
        "    for (int i = 0; i < N; ++i)\n",
        "    {\n",
        "        h_input[i] = 1; // 初始化数组为1\n",
        "    }\n",
        "\n",
        "    thrust::device_vector<TYPE> d_input(h_input, h_input + N);\n",
        "    thrust::device_vector<TYPE> d_output(ceil(N / (BLOCK_SIZE * 1.0)), 0);\n",
        "\n",
        "    int threads_per_block = BLOCK_SIZE;\n",
        "    int no_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "    reduce_0<<<no_of_blocks, threads_per_block, BLOCK_SIZE * sizeof(TYPE)>>>(thrust::raw_pointer_cast(d_input.data()), thrust::raw_pointer_cast(d_output.data()), int(N));\n",
        "    reduce_1<<<no_of_blocks, threads_per_block, BLOCK_SIZE * sizeof(TYPE)>>>(thrust::raw_pointer_cast(d_input.data()), thrust::raw_pointer_cast(d_output.data()), int(N));\n",
        "    reduce_2<<<no_of_blocks, threads_per_block, BLOCK_SIZE * sizeof(TYPE)>>>(thrust::raw_pointer_cast(d_input.data()), thrust::raw_pointer_cast(d_output.data()), int(N));\n",
        "    reduce_3<<<no_of_blocks, threads_per_block / 2, BLOCK_SIZE / 2 * sizeof(TYPE)>>>(thrust::raw_pointer_cast(d_input.data()), thrust::raw_pointer_cast(d_output.data()), int(N));\n",
        "    reduce_4<int(BLOCK_SIZE)><<<no_of_blocks, threads_per_block / 2, BLOCK_SIZE / 2 * sizeof(TYPE)>>>(thrust::raw_pointer_cast(d_input.data()), thrust::raw_pointer_cast(d_output.data()), int(N));\n",
        "    reduce_5<int(BLOCK_SIZE)><<<no_of_blocks, threads_per_block / 2, BLOCK_SIZE / 2 * sizeof(TYPE)>>>(thrust::raw_pointer_cast(d_input.data()), thrust::raw_pointer_cast(d_output.data()), int(N));\n",
        "    reduce_6<int(BLOCK_SIZE), int(NUM_PER_THREAD)><<<no_of_blocks, threads_per_block / NUM_PER_THREAD, BLOCK_SIZE / NUM_PER_THREAD * sizeof(TYPE)>>>(thrust::raw_pointer_cast(d_input.data()), thrust::raw_pointer_cast(d_output.data()), int(N));\n",
        "    reduce_7<int(BLOCK_SIZE), int(NUM_PER_THREAD)><<<no_of_blocks, threads_per_block / NUM_PER_THREAD, BLOCK_SIZE / NUM_PER_THREAD * sizeof(TYPE)>>>(thrust::raw_pointer_cast(d_input.data()), thrust::raw_pointer_cast(d_output.data()), int(N));\n",
        "    reduce_8<int(BLOCK_SIZE), int(NUM_PER_THREAD)><<<no_of_blocks, threads_per_block / NUM_PER_THREAD>>>(thrust::raw_pointer_cast(d_input.data()), thrust::raw_pointer_cast(d_output.data()), int(N));\n",
        "\n",
        "    thrust::host_vector<TYPE> h_output = d_output;\n",
        "\n",
        "    // int final_sum = thrust::reduce(d_input.begin(), d_input.end(), 0, thrust::plus<int>());\n",
        "\n",
        "    int final_sum = 0;\n",
        "    for (int i = 0; i < h_output.size(); ++i)\n",
        "    {\n",
        "        final_sum += h_output[i];\n",
        "    }\n",
        "\n",
        "    std::cout << \"Sum: \" << final_sum << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaI3nKjXRGj_",
        "outputId": "49414ed3-036f-4658-b40f-507a76cc456c"
      },
      "outputs": [],
      "source": [
        "!nvcc -o reduction -lineinfo reduction.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmdXU4E0TWmw",
        "outputId": "b611de1c-b0f4-42a0-979b-f716593eaee3"
      },
      "outputs": [],
      "source": [
        "!./reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UANfu1eBeV1Z",
        "outputId": "72166688-715b-4164-ff46-924e4f82e079"
      },
      "outputs": [],
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2024.2.3_2024.2.3.38-1_amd64.deb\n",
        "!apt update\n",
        "!apt install ./nsight-systems-2024.2.3_2024.2.3.38-1_amd64.deb\n",
        "!apt --fix-broken install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z0eK_edejbz"
      },
      "outputs": [],
      "source": [
        "!nsys profile -o report_nsys_reduction ./reduction -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVSGcOMWevak",
        "outputId": "7406da21-ab9a-423f-f862-5dd2b466b366"
      },
      "outputs": [],
      "source": [
        "!ncu --set full --replay-mode kernel --target-processes all -o report_ncu_reduction -f ./reduction"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
