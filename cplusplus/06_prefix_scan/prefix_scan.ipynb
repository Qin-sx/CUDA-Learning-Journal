{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adawlgCCMu2C"
   },
   "source": [
    "Reference: https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda  \n",
    "https://bbs.huaweicloud.com/blogs/323350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfY-q5iRMuQ_"
   },
   "source": [
    "prefix_scan_0_with_sysn (Hillis-Steele): used __gpu_sync function in kernel function for syschronization. It needs extra global variable and sleep function to barrier all block.  \n",
    "Another way: used other global variable to achieve offset addition in all blocks.  \n",
    "prefix_scan_0 (Hillis-Steele)\n",
    "\n",
    "## Hillis-Steele Prefix Scan Algorithm\n",
    "\n",
    "### Initial State\n",
    "\n",
    "Before any operations, the output array is initialized to:\n",
    "\n",
    "| Index | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
    "|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n",
    "| Value | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
    "\n",
    "### Step 1: s = 1\n",
    "\n",
    "Each element adds the value of its immediate left neighbor (**i - 1**):\n",
    "\n",
    "| Index  | 0      | 1      | 2      | 3      | 4      | 5      | 6      | 7      |\n",
    "|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n",
    "| Operation | -      | 0 + 1  | 1 + 2  | 2 + 3  | 3 + 4  | 4 + 5  | 5 + 6  | 6 + 7  |\n",
    "| Result | 0      | 1      | 3      | 5      | 7      | 9      | 11     | 13     |\n",
    "\n",
    "\n",
    "### Step 2: s = 2\n",
    "\n",
    "Each element adds the value two positions to its left (**i - 2**):\n",
    "\n",
    "| Index  | 0 | 1 | 2        | 3        | 4        | 5        | 6        | 7        |\n",
    "|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n",
    "| Operation | - | - | 0 + (1 + 2) | (0 + 1) + (2 + 3) | (1 + 2) + (3 + 4) | (2 + 3) + (4 + 5) | (3 + 4) + (5 + 6) | (4 + 5) + (6 + 7) |\n",
    "| Result | 0 | 1 | 3        | 6        | 10       | 14       | 18       | 22       |\n",
    "\n",
    "### Step 3: s = 4\n",
    "\n",
    "Each element adds the value four positions to its left (**i - 4**):\n",
    "\n",
    "| Index  | 0 | 1 | 2 | 3 | 4                | 5                | 6                | 7                |\n",
    "|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n",
    "| Operation | - | - | - | - | 0 + (1 + 2 + 3 + 4) | (0 + 1) + (2 + 3 + 4 + 5) | (0 + 1 + 2) + (3 + 4 + 5 + 6)| (0 + 1 + 2 + 3) + (4 + 5 + 6 + 7) |\n",
    "| Result | 0 | 1 | 3 | 6 | 10               | 15               | 21               | 28               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnNsWrWNMjl0",
    "outputId": "24bf1c25-3bd3-4b65-e056-21a6559947a9"
   },
   "outputs": [],
   "source": [
    "%%writefile prefix_scan.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <type_traits>\n",
    "#include <thrust/host_vector.h>\n",
    "#include <thrust/device_vector.h>\n",
    "#include <thrust/sequence.h>\n",
    "// #include <cooperative_groups.h>\n",
    "// #include <cuda_runtime_api.h>\n",
    "\n",
    "\n",
    "#define TYPE int\n",
    "#define N 20240\n",
    "#define BLOCK_SIZE 1024\n",
    "#define NUM_PER_THREAD 8\n",
    "#define WARP_SIZE (BLOCK_SIZE / NUM_PER_THREAD / 32)\n",
    "\n",
    "// namespace cg = cooperative_groups;\n",
    "\n",
    "// lock-based\n",
    "__device__ volatile int g_mutex[1024];\n",
    "\n",
    "// GPU lock-based synchronization function\n",
    "__device__ void __gpu_sync(int goalVal)\n",
    "{\n",
    "  // thread ID in a block\n",
    "  int tid_in_block = threadIdx.x * blockDim.y + threadIdx.y;\n",
    "  int bid_in_grid = blockIdx.x * gridDim.y + blockIdx.y;\n",
    "\n",
    "  // only thread 0 is used for synchronization\n",
    "  if (tid_in_block == 0)\n",
    "  {\n",
    "    for(int i = 0; i < gridDim.x * gridDim.y; ++i)\n",
    "      atomicAdd((int*) &(g_mutex[i]), 1);\n",
    "\n",
    "    // only when all blocks add 1 to g_mutex\n",
    "    // will g_mutex equal to goalVal\n",
    "    while (g_mutex[bid_in_grid] != goalVal)\n",
    "    {\n",
    "      // Do nothing here\n",
    "    }\n",
    "    g_mutex[bid_in_grid] = 0;\n",
    "  }\n",
    "  __syncthreads();\n",
    "\n",
    "  unsigned int count = 0;\n",
    "  while (count < 100) {\n",
    "      count++;\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "__global__ void  warm_up()\n",
    "{\n",
    "    int indexX = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (indexX < N)\n",
    "    {\n",
    "        float a = 0.0f;\n",
    "        float b = 1.0f;\n",
    "        float c = a + b;\n",
    "    }\n",
    "}\n",
    "\n",
    "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
    "void prefix_scan(T* input, T* output, int n)\n",
    "{\n",
    "    output[0] = input[0];\n",
    "    for (int i = 1; i < n; ++i)\n",
    "    {\n",
    "        output[i] = output[i - 1] + input[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
    "__global__ void prefix_scan_0_with_sysn(T* input, T* output, int n)\n",
    "{\n",
    "    extern __shared__ T sdata[];\n",
    "\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    int tid = threadIdx.x;\n",
    "\n",
    "    if (idx < n)\n",
    "    {\n",
    "        sdata[tid] = input[idx];\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        sdata[tid] = 0;\n",
    "    }\n",
    "\n",
    "    __syncthreads();\n",
    "\n",
    "    for (unsigned int s = 1; s < blockDim.x; s *= 2)\n",
    "    {\n",
    "        T temp = 0;\n",
    "        if (tid >= s)\n",
    "        {\n",
    "            temp = sdata[tid - s];\n",
    "        }\n",
    "        __syncthreads();\n",
    "        sdata[tid] += temp;\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (idx < n)\n",
    "    {\n",
    "        output[idx] = sdata[tid];\n",
    "    }\n",
    "\n",
    "    // Get the thread block and grid\n",
    "    // cg::thread_block cta = cg::this_thread_block();\n",
    "    // cg::grid_group grid = cg::this_grid();\n",
    "\n",
    "    printf(\"blockIdx.x: %d, gridDim.x: %d\\n\", blockIdx.x, gridDim.x);\n",
    "    for(int i = 1; i < gridDim.x*gridDim.y; ++i)\n",
    "    {\n",
    "        // __threadfence();// It doesn't work\n",
    "        // grid.sync();// It doesn't work\n",
    "        __gpu_sync(gridDim.x*gridDim.y);\n",
    "        T offset = output[i * blockDim.x - 1];\n",
    "        if (idx < n && blockIdx.x == i)\n",
    "        {\n",
    "            output[idx] += offset;\n",
    "            printf(\"i: %d idx: %d blockIdx.x: %d, gridDim.x: %d, output: %d\\n\",i, idx, blockIdx.x, gridDim.x, output[idx]);\n",
    "        }\n",
    "\n",
    "        // __threadfence();\n",
    "        // grid.sync(); // It doesn't work\n",
    "    }\n",
    "}\n",
    "\n",
    "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
    "__global__ void prefix_scan_0(T* input, T* output, int n)\n",
    "{\n",
    "    extern __shared__ T sdata[];\n",
    "\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    int tid = threadIdx.x;\n",
    "\n",
    "    if (idx < n)\n",
    "    {\n",
    "        sdata[tid] = input[idx];\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        sdata[tid] = 0;\n",
    "    }\n",
    "\n",
    "    for (unsigned int s = 1; s < blockDim.x; s *= 2)\n",
    "    {\n",
    "        __syncthreads();\n",
    "        T temp = 0;\n",
    "        if (tid >= s)\n",
    "        {\n",
    "            temp = sdata[tid - s];\n",
    "        }\n",
    "        __syncthreads();\n",
    "        sdata[tid] += temp;\n",
    "\n",
    "    }\n",
    "\n",
    "    if (idx < n)\n",
    "    {\n",
    "        output[idx] = sdata[tid];\n",
    "        // printf(\"idx: %d blockIdx.x: %d, output: %d\\n\",idx, blockIdx.x, output[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
    "__global__ void cal_offset(T* output, T* offset)\n",
    "{\n",
    "    int tid = threadIdx.x;\n",
    "    int bid = blockIdx.x;\n",
    "\n",
    "    offset[bid] = 0;\n",
    "\n",
    "    if (tid == 0)\n",
    "    {\n",
    "        for(int i = 1; i <= blockIdx.x; ++i)\n",
    "        {\n",
    "          offset[bid] += output[i * blockDim.x - 1];\n",
    "        }\n",
    "        // printf(\"offset[bid], %d, bid, %d\\n\", offset[bid], bid);\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "template <typename T, typename = std::enable_if_t<std::is_arithmetic<T>::value>>\n",
    "__global__ void add_offset(T* output, T* offset, int n)\n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (idx < n)\n",
    "    {\n",
    "        output[idx] += offset[blockIdx.x];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    TYPE h_input[N];\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        h_input[i] = 1;\n",
    "    }\n",
    "\n",
    "    TYPE h_output[N];\n",
    "    prefix_scan(h_input, h_output, N);\n",
    "\n",
    "\n",
    "    thrust::device_vector<TYPE> d_input(h_input, h_input + N);\n",
    "    thrust::device_vector<TYPE> d_output(N);\n",
    "\n",
    "    int threads_per_block = BLOCK_SIZE;\n",
    "    int no_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
    "\n",
    "    warm_up<<<no_of_blocks, threads_per_block>>>();\n",
    "    prefix_scan_0<<<no_of_blocks, threads_per_block, threads_per_block * sizeof(TYPE)>>>(thrust::raw_pointer_cast(d_input.data()), thrust::raw_pointer_cast(d_output.data()), int(N));\n",
    "\n",
    "    thrust::device_vector<TYPE> d_offset(no_of_blocks);\n",
    "    cal_offset<<<no_of_blocks, threads_per_block>>>(thrust::raw_pointer_cast(d_output.data()), thrust::raw_pointer_cast(d_offset.data()));\n",
    "    add_offset<<<no_of_blocks, threads_per_block>>>(thrust::raw_pointer_cast(d_output.data()), thrust::raw_pointer_cast(d_offset.data()), int(N));\n",
    "\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    thrust::host_vector<TYPE> h_result = d_output;\n",
    "\n",
    "    bool match = true;\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        if (h_output[i] != h_result[i])\n",
    "        {\n",
    "            std::cout << i << \" \" << h_output[i] << \" \" << h_result[i] << std::endl;\n",
    "            match = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if (match)\n",
    "        std::cout << \"Results match!\" << std::endl;\n",
    "    else\n",
    "        std::cout << \"Results do not match!\" << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h84fWJIyOorG",
    "outputId": "42623057-eaee-4d3f-9bb8-b69ca9595608"
   },
   "outputs": [],
   "source": [
    "!nvcc -o prefix_scan -lineinfo prefix_scan.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CI_QoDRYPOZa",
    "outputId": "a517d87d-7be2-4794-81a4-ca253c08db82"
   },
   "outputs": [],
   "source": [
    "!./prefix_scan"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
